{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6f80db-1475-47fd-89fa-1ec535699ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer \n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset \n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import evaluate\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35ed087-059f-406b-9245-644f074efb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading tokenizer...]\n",
      "[Loading tokenizer complete]\n",
      "[Loading tokenizer...]\n",
      "[Loading tokenizer complete]\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "    # print(type(dataset))\n",
    "    # print(type(dataset[\"train\"]))\n",
    "    # print(type(dataset[\"train\"][0]))\n",
    "    # print((dataset[\"train\"][0]))\n",
    "    # print()\n",
    "print(\"[Loading tokenizer...]\")\n",
    "dataset = load_dataset(\"json\", data_files = \"kick_train.json\")\n",
    "print(\"[Loading tokenizer complete]\")\n",
    "\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(\"[Loading tokenizer...]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"KT-AI/midm-bitext-S-7B-inst-v1\",\n",
    "        trust_remote_code = True\n",
    "    )\n",
    "print(\"[Loading tokenizer complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1116524e-10d1-4886-b9f2-8501738ffe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizing dataset...]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f33bc71ec74659b3ac14e680a5a7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizing dataset complete]\n"
     ]
    }
   ],
   "source": [
    "# dummy_data = \"### User; I see a korean report of a chemical industry, and some text field says '개스킷 교체 필요함' in a container of a chemical container. What does this mean, and what should I do? \\n ### Midm;\"\n",
    "    \n",
    "# start = time.time()\n",
    "# data = tokenizer(dummy_data, return_tensors = \"pt\")\n",
    "# end = time.time()\n",
    "# sec = (end - start)\n",
    "# result = datetime.timedelta(seconds = sec)\n",
    "# print(\"Tokenizing Time: \", result)\n",
    "\n",
    "# print(data)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], padding = \"max_length\", truncation = True)\n",
    "    return tokenizer(examples[\"input\"], return_tensors = \"pt\", padding = \"max_length\", truncation = True, max_length=697)\n",
    "\n",
    "print(\"[Tokenizing dataset...]\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched = True)\n",
    "print(\"[Tokenizing dataset complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bbc028-066e-435a-9f56-0d150dc22c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sampling dataset...]\n",
      "[Sampling dataset complete]\n",
      "\t <class 'datasets.arrow_dataset.Dataset'>\n",
      "\t (10000, 5)\n",
      "\t <class 'datasets.arrow_dataset.Dataset'>\n",
      "\t (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"[Sampling dataset...]\")\n",
    "# train_dataset = tokenized_dataset[\"train\"].shuffle(seed = 42).select( range( int(len(tokenized_dataset[\"train\"]) * 0.9) ) )\n",
    "# eval_range = range( int(len(tokenized_dataset[\"train\"]) * 0.1) - 1 )\n",
    "# eval_range = [ i + int(len(tokenized_dataset[\"train\"]) * 0.9) for i in eval_range ]\n",
    "# eval_dataset = tokenized_dataset[\"train\"].shuffle(seed = 42).select( eval_range )\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed = 42).select( range( 10000 ) )\n",
    "eval_range = [ i + 10000 for i in range(1000) ]\n",
    "eval_dataset = tokenized_dataset[\"train\"].shuffle(seed = 42).select( eval_range )\n",
    "print(\"[Sampling dataset complete]\")\n",
    "\n",
    "print(\"\\t\", type(train_dataset))\n",
    "print(\"\\t\", train_dataset.shape)\n",
    "print(\"\\t\", type(eval_dataset))\n",
    "print(\"\\t\", eval_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b213b264-1ed5-4c6e-bbc9-6635b00e20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading model...]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149119e24665497ab2912919e93aede4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading model complete]\n",
      "[Model cuda...]\n",
      "[Model cuda complete]\n",
      "[Model train...]\n",
      "[Model train complete]\n"
     ]
    }
   ],
   "source": [
    "    # model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "print(\"[Loading model...]\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"KT-AI/midm-bitext-S-7B-inst-v1\", \n",
    "        trust_remote_code = True\n",
    "    )\n",
    "print(\"[Loading model complete]\")\n",
    "\n",
    "print(\"[Model cuda...]\")\n",
    "model.cuda()\n",
    "print(\"[Model cuda complete]\")\n",
    "\n",
    "print(\"[Model train...]\")\n",
    "model.train()\n",
    "print(\"[Model train complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a9a636-ea87-493d-b6cb-20e00d1cf383",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training_args = TrainingArguments(output_dir = \"/root/fine_tuned_midm_checkpoints\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"/root/fine_tuned_midm_checkpoints\", \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "    # metric = evaluate.load(\"accuracy\")\n",
    "    # def compute_metrics(eval_pred):\n",
    "    #     logits, labels = eval_pred\n",
    "    #     predictions = np.argmax(logits, axis = -1)\n",
    "        \n",
    "    #     return metric.compute(predictions = predictions, references=labels)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8e26f0-fa11-4014-9dff-3ba4340cdd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    loss = loss_fn(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2237791c-3c39-4bf8-ba60-c12e5c55e3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trainer definition...]\n",
      "[Trainer definition complete]\n"
     ]
    }
   ],
   "source": [
    "print(\"[Trainer definition...]\")\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    \n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "print(\"[Trainer definition complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea210c2b-2c54-4b24-8e54-108e747e10b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training...]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Training...]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Training complete]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m exit()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer.py:2765\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2765\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2767\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2768\u001b[0m         )\n\u001b[1;32m   2769\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "print(\"[Training...]\")\n",
    "trainer.train()\n",
    "print(\"[Training complete]\")\n",
    "\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b27406e-0afa-4afd-a6e6-8d2cc4a6bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송하지만, 제가 제공할 수 있는 정보가 없습니다. 1300년도에 세종대왕이 그리스를 방문했다는 정보는 없습니다. 더 자세한 정보를 제공해 주실 수 있나요?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "anrg=\"\"\"1300년도에 세종대왕이 그리스를 방문했던 사건이 뭐였지?\"\"\"\n",
    "dummy_data = f\"###User;{anrg}\\n###Midm;\"\n",
    "data = tokenizer(dummy_data, return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "pred = model.generate(\n",
    "    input_ids=data.input_ids[..., :-1].cuda(),\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=float('inf')\n",
    ")\n",
    "decoded_text = tokenizer.batch_decode(pred[0], skip_special_tokens=True)\n",
    "model.train()\n",
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
