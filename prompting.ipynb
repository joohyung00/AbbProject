{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6f80db-1475-47fd-89fa-1ec535699ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer \n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset \n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import evaluate\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35ed087-059f-406b-9245-644f074efb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68920\n",
      "7656\n"
     ]
    }
   ],
   "source": [
    "with open(\"kick_train.json\", \"r\") as file:\n",
    "    dataset_json = json.load(file)\n",
    "\n",
    "train_length = int(len(dataset_json) * 0.9) - 1\n",
    "eval_length = int(len(dataset_json) * 0.1) - 1\n",
    "\n",
    "train_dataset = dataset_json[:train_length]\n",
    "eval_dataset = dataset_json[train_length:train_length + eval_length]\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0853c27c-6de9-4b69-adf3-33535d168dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"다음은 임의의 프로젝트에 대한 정보이다. 프로젝트의 정보를 바탕으로 해당 프로젝트가 '성공'하였는지, 또는 '실패'하였는지 예상하라. '성공' 또는 '실패'로만 대답하라.\", 'input': '본 프로젝트의 이름은 LOS ANGELES KOREA TOWN MURAL PROJECT이다. 프로젝트의 키워드는 los-angeles-korea-town-mural-project이며, 구체적인 설명은 다음과 같다 - The LA Korea Town Mural Project would create a mural of traditional Korean culture and history in the Los Angeles Korea Town area. 프로젝트 책임자의 국적은 US이며, 목표 금액은 30000.0 USD이다. 본 프로젝트의 런칭 시각은 1356654814이며, due date는 1359246814이다. 본 프로젝트는 지지자들의 수는 5 명이며, 지지자들과의 온라인 커뮤니케이션을 허용하지 않았다.', 'output': '실패'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11df12a0-eb29-4d09-866d-c8c9c03d3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading tokenizer...]\n",
      "[Loading tokenizer complete]\n"
     ]
    }
   ],
   "source": [
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(\"[Loading tokenizer...]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"KT-AI/midm-bitext-S-7B-inst-v1\",\n",
    "        trust_remote_code = True\n",
    "    )\n",
    "print(\"[Loading tokenizer complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b213b264-1ed5-4c6e-bbc9-6635b00e20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading model...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1:\n",
      "- configuration_midm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1:\n",
      "- rotary_position_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1:\n",
      "- modeling_midm.py\n",
      "- rotary_position_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc62490576b4e39819c1f36757df65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28573d39ea324996a4ca8f6adbb4f4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading model complete]\n",
      "[Model cuda...]\n",
      "[Model cuda complete]\n",
      "[Model train...]\n",
      "[Model train complete]\n"
     ]
    }
   ],
   "source": [
    "    # model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "print(\"[Loading model...]\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"KT-AI/midm-bitext-S-7B-inst-v1\", \n",
    "        trust_remote_code = True\n",
    "    )\n",
    "print(\"[Loading model complete]\")\n",
    "\n",
    "print(\"[Model cuda...]\")\n",
    "model.cuda()\n",
    "print(\"[Model cuda complete]\")\n",
    "\n",
    "print(\"[Model train...]\")\n",
    "model.eval()\n",
    "print(\"[Model train complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b27406e-0afa-4afd-a6e6-8d2cc4a6bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfind(list_, target):\n",
    "    index = None\n",
    "    for i, element in enumerate(list_):\n",
    "        if element == target:\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "def find(list_, target):\n",
    "    for i, element in enumerate(list_):\n",
    "        if element == target:\n",
    "            return i\n",
    "    return None\n",
    "    \n",
    "\n",
    "def extractSubstring(input_list):\n",
    "    start_index = rfind(input_list, \";\") + 1\n",
    "    end_index = rfind(input_list, \"</s>\")\n",
    "    return input_list[start_index : end_index]\n",
    "\n",
    "\n",
    "def predictNoShot(prompt, data, label):\n",
    "\n",
    "    anrg = prompt + data\n",
    "    question = f\"###User;{anrg}\\n###Midm;\"\n",
    "    data = tokenizer(question, return_tensors = \"pt\")\n",
    "    \n",
    "    # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    pred = model.generate(\n",
    "        input_ids = data.input_ids[..., : -1].cuda(),\n",
    "        # streamer = streamer,\n",
    "        use_cache = True,\n",
    "        max_new_tokens = float('inf')\n",
    "    )\n",
    "    decoded_text = tokenizer.batch_decode(pred[0], skip_special_tokens = True)\n",
    "\n",
    "    # between ; and </s>\n",
    "    # print(decoded_text)\n",
    "    extract_substring_list = extractSubstring(decoded_text)\n",
    "    # print(extract_substring_list)\n",
    "    prediction = \" \".join(extract_substring_list)\n",
    "\n",
    "    if label == \"성공\" and \"실패\" not in prediction:\n",
    "        return True\n",
    "    if label == \"실패\" and \"실패\" in prediction:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# prompt = \"다음은 임의의 프로젝트에 대한 정보이다. 프로젝트의 정보를 바탕으로 해당 프로젝트가 성공하였는지, \\\n",
    "# 또는 실패하였는지 예상하라('성공' 또는 '실패'로만 대답하라). \\n\"\n",
    "# anrg = \"본 프로젝트의 이름은 LOS ANGELES KOREA TOWN MURAL PROJECT이다. \\\n",
    "# 프로젝트의 키워드는 los-angeles-korea-town-mural-project이며, 구체적인 설명은 다음과 같다 - The LA Korea Town Mural Project \\\n",
    "# would create a mural of traditional Korean culture and history in the Los Angeles Korea Town area. 프로젝트 책임자의 국적은 \\\n",
    "# US이며, 목표 금액은 30000.0 USD이다. 본 프로젝트의 런칭 시각은 1356654814이며, due date는 1359246814이다. 본 프로젝트는 지지자들의 \\\n",
    "# 수는 5 명이며, 지지자들과의 온라인 커뮤니케이션을 허용하지 않았다.\"\n",
    "\n",
    "# predictNoShot(prompt, anrg, \"실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45275b23-665b-4696-89fa-e832915cd0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▊                                                                  | 615/7656 [09:35<1:49:48,  1.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m info \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredictNoShot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m성공\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result: true_positive \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[41], line 28\u001b[0m, in \u001b[0;36mpredictNoShot\u001b[0;34m(prompt, data, label)\u001b[0m\n\u001b[1;32m     25\u001b[0m data \u001b[38;5;241m=\u001b[39m tokenizer(question, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# streamer = streamer,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(pred[\u001b[38;5;241m0\u001b[39m], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# between ; and </s>\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(decoded_text)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/generation/utils.py:2578\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2573\u001b[0m     unfinished_sequences \u001b[38;5;241m=\u001b[39m unfinished_sequences\u001b[38;5;241m.\u001b[39mmul(\n\u001b[1;32m   2574\u001b[0m         next_tokens\u001b[38;5;241m.\u001b[39mtile(eos_token_id_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mne(eos_token_id_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mprod(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;66;03m# stop when each sentence is finished\u001b[39;00m\n\u001b[0;32m-> 2578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unfinished_sequences\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2579\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NO SHOT\n",
    "\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "for data in tqdm(eval_dataset):\n",
    "    prompt = data[\"instruction\"]\n",
    "    info = data[\"input\"]\n",
    "    label = data[\"output\"]\n",
    "\n",
    "    result = predictNoShot(prompt, info, label)\n",
    "\n",
    "    if label == \"성공\":\n",
    "        if result: true_positive += 1\n",
    "        else:      false_negative += 1\n",
    "    else:\n",
    "        if result: true_negative += 1\n",
    "        else:      false_positive += 1\n",
    "\n",
    "tp = true_positive\n",
    "tn = true_negative\n",
    "fp = false_positive\n",
    "fn = false_negative\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9efb5-f55d-4ab2-94c7-4f44835edce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
